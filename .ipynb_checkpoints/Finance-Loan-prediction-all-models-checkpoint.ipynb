{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Repayment Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as  plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Data & statistical analysis\n",
    "def load_data():\n",
    "    train=pd.read_csv('loan.csv')\n",
    "    print(train.shape)\n",
    "    print(train.info())\n",
    "    print(train.describe().T)\n",
    "    \n",
    "    #Categorical Features\n",
    "    cat_features=train.select_dtypes(include='object').columns\n",
    "    print(\"Categorical Features:\\n\",cat_features,\"\\n No of categorical features:\",len(cat_features))\n",
    "    \n",
    "    #Numerical Features\n",
    "    num_features=train.select_dtypes(exclude='object').columns\n",
    "    print(\"Numerical Features:\\n\",num_features,\"\\n No of numerical features:\",len(num_features))\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Anaysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploratory_analysis(train):\n",
    "    #Create copy of dataframe \n",
    "    df=train.copy()\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    #Count of target Feature\n",
    "    print(train['bad_loan'].value_counts())\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(\"Count of bad_loan\")\n",
    "    sns.countplot(train['bad_loan'],palette='plasma')\n",
    "    plt.xticks(ticks=np.arange(2),labels=['Good Customer','Bad Customer'])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    train['bad_loan'].value_counts().plot(kind='pie',labels=['Good Customers','Bad Customers'],autopct=\"%1.2f%%\")\n",
    "    plt.show()\n",
    "    #Conclusion: Dataset is imbalanced as bad_customers are less than good customers\n",
    "    \n",
    "    #Count of target Feature\n",
    "    print(train['bad_loan'].value_counts())\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(\"Count of bad_loan\")\n",
    "    sns.countplot(train['bad_loan'],palette='plasma')\n",
    "    plt.xticks(ticks=np.arange(2),labels=['Good Customer','Bad Customer'])\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(5,5))\n",
    "    train['bad_loan'].value_counts().plot(kind='pie',labels=['Good Customers','Bad Customers'],autopct=\"%1.2f%%\")\n",
    "    plt.show()\n",
    "    #Conclusion: Dataset is imbalanced as bad_customers are less than good customers\n",
    "    \n",
    "    #Distribution of loan amount\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(\"Histogram for Loan Amount \")\n",
    "    plt.hist(df['loan_amnt'])\n",
    "    plt.xlabel(\"Loan Amount\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.distplot(df['loan_amnt'])\n",
    "    #Conclusion:Major customers took loan in range of(5000-10000), few customers took loan with amoun as 30000-35000\n",
    "    \n",
    "    #Countplot of term \n",
    "    print(train['term'].value_counts())\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.title(\"Countplot for term\")\n",
    "    sns.countplot(train['term'],palette='plasma')\n",
    "    plt.xticks(ticks=np.arange(2),labels=['36 Months','60 Months'])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    train['term'].value_counts().plot(kind='pie',labels=['36 Months','60 Months'],autopct=\"%1.2f%%\")\n",
    "    plt.show()\n",
    "    #Conclusion: Majority of loans are of 36 Month duration\n",
    "    \n",
    "    #Countplot for emp_length Feature\n",
    "    print(\"EmpLength\\tNo of customers\")\n",
    "    print(train['emp_length'].value_counts())\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(\"Countplot for Employee Length\")\n",
    "    sns.countplot(train['emp_length'],palette='plasma')\n",
    "    plt.xticks(ticks=np.arange(11))\n",
    "    plt.show()\n",
    "    #Conclusion: Majority of customers having employee length as 10\n",
    "    \n",
    "    #Relationship between Loan Amount and Annual Income\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(df['loan_amnt'],np.log(df['annual_inc']))\n",
    "    plt.title(\"Annual Income vs Loan Amount\")\n",
    "    plt.ylabel(\"Loan Amount\")\n",
    "    plt.xlabel(\"Annual Income\")\n",
    "\n",
    "    #Distribution of Annual Income\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(\"Distribution Annual Income\")\n",
    "    plt.hist(df['annual_inc'],bins=20)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel(\"Annual Income\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.distplot(df['annual_inc'],bins=20)\n",
    "\n",
    "    #As distribution is skewed apply log transformation on it\n",
    "    #Distribution of Annual Income\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(\"Distribution Annual Income\")\n",
    "    plt.hist(np.log(df['annual_inc']),bins=20)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel(\"Annual Income\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.distplot(np.log(df['annual_inc']),bins=20)\n",
    "    #Conclusion:Major customers took loan in range of(5000-10000), few customers took loan with amoun as 30000-35000\n",
    "\n",
    "    #Count of loan purpose\n",
    "    print(train['purpose'].value_counts())\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.title(\"Countplot for Loan Purpose\")\n",
    "    sns.countplot(train['purpose'],palette='plasma')\n",
    "    plt.xlabel(\"Loan Purpose\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    #Conclusion: Majority of customers take loan for debit consolidation followed for credit card\n",
    "    \n",
    "    #Statewise customer count\n",
    "    print(train['addr_state'].value_counts())\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.title(\"Statewise Customer Count\")\n",
    "    sns.countplot(train['addr_state'],palette='plasma')\n",
    "    plt.xlabel(\"State\")\n",
    "    plt.ylabel(\"Customer Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    #Conclusion:  state CA has majority  of customers , most of the states have average no of customers\n",
    "    \n",
    "    #Distribution of Debit to Income Ratio\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(\"Distribution Debit-Income\")\n",
    "    sns.distplot(df['dti'])\n",
    "    plt.xlabel(\"Debit-Income Ratio\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "    #Distribution of Revol_util\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(\"Distribution Revol_Util\")\n",
    "    sns.distplot(df['revol_util'])\n",
    "    plt.xlabel(\"Revol_Util\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "    \n",
    "    #Distribution of total account \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(\"Histogram for total account\")\n",
    "    plt.hist(df['total_acc'])\n",
    "    plt.xlabel(\"Total Account\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()  \n",
    "    \n",
    "    #Count of Longest Credit length\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(\"Countplot for Longest Credit Length\")\n",
    "    sns.distplot(df['longest_credit_length'])\n",
    "    plt.show()\n",
    "\n",
    "    #Count of House Ownership\n",
    "    print(train['home_ownership'].value_counts())\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(\"Countplot of House Ownership\")\n",
    "    sns.countplot(train['home_ownership'])\n",
    "    plt.xticks(ticks=np.arange(6),labels=['MORTGAGE','RENT','OWN','OTHER','NONE','ANY'])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    train['home_ownership'].value_counts().plot(kind='pie',labels=['MORTGAGE','RENT','OWN','OTHER','NONE','ANY'],autopct=\"%1.2f%%\")\n",
    "    plt.show()    \n",
    "  \n",
    "    #Count of Verified and Not verified Customers\n",
    "    print(train['verification_status'].value_counts())\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title(\"Countplot by Verification Status\")\n",
    "    sns.countplot(train['verification_status'],palette='plasma')\n",
    "    plt.xticks(ticks=np.arange(2),labels=['Verified','Not Verified'])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    train['verification_status'].value_counts().plot(kind='pie',labels=['Verified','Not Verified'],autopct=\"%1.2f%%\")\n",
    "    plt.show()\n",
    "\n",
    "    #Correlation of features\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(df.corr(),annot=True,cbar=True,cmap='viridis',linewidth=0.2)\n",
    "    plt.yticks(rotation=0)   \n",
    "    \n",
    "   #Checking for outlier\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.boxplot(y=np.log(df['annual_inc']))\n",
    "    #There exist extrem value for annual income \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engg(train):\n",
    "    #Filling Missing Values\n",
    "    print(train.isnull().sum())\n",
    "    #Employee Length:Fill na values with mean\n",
    "    train['emp_length'].fillna(train['emp_length'].mean(skipna=True),inplace=True)\n",
    "    \n",
    "    #Annual Income:mean income\n",
    "    train['annual_inc'].fillna(train['annual_inc'].median(),inplace=True)\n",
    "    \n",
    "    #delinq_2 yrs\n",
    "    train['delinq_2yrs'].fillna(train['delinq_2yrs'].mean(skipna=True),inplace=True)\n",
    "    \n",
    "    #revol_util\n",
    "    train['revol_util'].fillna(train['revol_util'].median(),inplace=True)\n",
    "    \n",
    "    #revol_util\n",
    "    train['total_acc'].fillna(train['total_acc'].median(),inplace=True)\n",
    "    \n",
    "    #longest_credit_length\n",
    "    train['longest_credit_length'].fillna(train['longest_credit_length'].median(),inplace=True)\n",
    "    print(train.isnull().sum())\n",
    "    \n",
    "    #Categorical Features\n",
    "    cat_features=train.select_dtypes(include='object').columns\n",
    "    print(\"Categorical Features:\\n\",cat_features,\"\\n No of categorical features:\",len(cat_features))\n",
    "    #Converting Categorical-Numeric features using Label Encoding\n",
    "    le=LabelEncoder()\n",
    "    for feature in cat_features:\n",
    "        train[feature]=le.fit_transform(train[feature])\n",
    "    \n",
    "    #Apply log transformation to deal with skewness of annual income\n",
    "    train['annual_inc']=np.log(train['annual_inc'])\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function call\n",
    "train=load_data()\n",
    "exploratory_analysis(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=feature_engg(train)\n",
    "print(train.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test data\n",
    "x=train.drop('bad_loan',axis=1)\n",
    "y=train.bad_loan\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)\n",
    "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)\n",
    "\n",
    "#Feature Scaling\n",
    "rs=RobustScaler()\n",
    "x_train=rs.fit_transform(x_train)\n",
    "x_test=rs.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without CV & without Feature Selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model=LogisticRegression(solver='liblinear')\n",
    "model.fit(x_train,y_train)\n",
    "y_pred=model.predict(x_test)\n",
    "print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"Classification Report:\\n\",classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With CV & without Feature Selection\n",
    "#Cross Validation\n",
    "sc=RobustScaler()\n",
    "x=sc.fit_transform(x)\n",
    "cv_score=cross_val_score(model,x,y,cv=10)\n",
    "print(\"Accuracy with Cross Validation:\",np.mean(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection:Pearson's Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection : Correlation matrix\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(train.corr(),annot=True,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection using correlation coefficient\n",
    "correlated_features = set()\n",
    "irrelevant_features=set()\n",
    "correlation_matrix = train.corr()\n",
    "'''\n",
    "#Checking entire correlation matrix\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "        if abs(correlation_matrix.iloc[i, j]) < 0.05:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            irrelevant_features.add(colname)\n",
    "'''\n",
    "#Checking Correlation of all features with respect to target feature\n",
    "i=12   #index of target feature\n",
    "for j in range(0,15):\n",
    "    if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "        colname = correlation_matrix.columns[j]\n",
    "        correlated_features.add(colname)\n",
    "    if abs(correlation_matrix.iloc[i, j]) < 0.05:\n",
    "        colname1 = correlation_matrix.columns[j]\n",
    "        irrelevant_features.add(colname1)            \n",
    "print(\"Correlated features are as: {} ,Total No of Correlated features:{}\".format(correlated_features,len(correlated_features)))\n",
    "print(\"Irrelevant features are as: {} ,Total No of Irrelevant features:{}\".format(irrelevant_features,len(irrelevant_features)))\n",
    "\n",
    "#Creates a copy of dataset\n",
    "df=train.copy()\n",
    "#Dropping irrelevant features\n",
    "df.drop(irrelevant_features,axis=1,inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting of dataset - train,test data after dropping irrelevant features\n",
    "x1=df.drop('bad_loan',axis=1)\n",
    "y1=df.bad_loan\n",
    "x_train1,x_test1,y_train1,y_test1=train_test_split(x1,y1,test_size=0.2,random_state=0)\n",
    "print(x_train1.shape,y_train1.shape,x_test1.shape,y_test1.shape)\n",
    "\n",
    "#Feature Scaling\n",
    "rs=RobustScaler()\n",
    "x_train1=rs.fit_transform(x_train1)\n",
    "x_test1=rs.transform(x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building model : Logistic Regression\n",
    "#Without CV & with Feature Selection\n",
    "model.fit(x_train1,y_train1)\n",
    "y_pred=model.predict(x_test1)\n",
    "print(\"Accuracy:\",accuracy_score(y_test1,y_pred))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test1,y_pred))\n",
    "print(\"Confusion Matrix:\\n\",classification_report(y_test1,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation\n",
    "#With CV & With Feature Selection\n",
    "x1=sc.fit_transform(x1)\n",
    "cv_score=cross_val_score(model,x1,y1,cv=10)\n",
    "print(\"Accuracy with Cross Validation:\",np.mean(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without CV & without Feature Selection\n",
    "from sklearn import tree\n",
    "dtree = tree.DecisionTreeClassifier(random_state=17)\n",
    "dtree.fit(x_train,y_train)\n",
    "y_pred=dtree.predict(x_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without CV & with Feature Selection\n",
    "cv_score=cross_val_score(dtree,x,y,cv=10)\n",
    "print(\"Accuracy:\",np.mean(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without CV & with Feature Selection\n",
    "dtree.fit(x_train1,y_train1)\n",
    "y_pred=dtree.predict(x_test1)\n",
    "print(accuracy_score(y_test1,y_pred))\n",
    "print(confusion_matrix(y_test1,y_pred))\n",
    "print(classification_report(y_test1,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With CV & with Feature Selection\n",
    "cv_score=cross_val_score(dtree,x1,y1,cv=10)\n",
    "print(\"Accuracy:\",np.mean(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADABoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without CV & without Feature Selection\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "adb=AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=100)\n",
    "adb.fit(x_train,y_train)\n",
    "y_pred=adb.predict(x_test)\n",
    "print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"Confusion Matrix:\\n\",classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without CV & without Feature Selection\n",
    "adb=AdaBoostClassifier()\n",
    "cv_score=cross_val_score(adb,x,y,cv=10)\n",
    "print(\"Accuracy:\",np.mean(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without CV &  with Feature selection\n",
    "adb.fit(x_train1,y_train1)\n",
    "y_pred=adb.predict(x_test1)\n",
    "print(\"Accuracy:\",accuracy_score(y_test1,y_pred))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test1,y_pred))\n",
    "print(\"Confusion Matrix:\\n\",classification_report(y_test1,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With CV & with Feature Selection\n",
    "cv_score=cross_val_score(adb,x1,y1,cv=10)\n",
    "print(\"Accuracy:\",np.mean(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without CV & without Feature Selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_train, y_train)\n",
    "y_pred= rfc.predict(x_test)\n",
    "print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"Confusion Matrix:\\n\",classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without CV & without Feature Selection\n",
    "cv_score=cross_val_score(rfc,x,y,cv=10)\n",
    "print(\"Accuracy:\",np.mean(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without CV & with Feature Selection\n",
    "rfc.fit(x_train1, y_train1)\n",
    "y_pred= rfc.predict(x_test1)\n",
    "print(\"Accuracy:\",accuracy_score(y_test1,y_pred))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test1,y_pred))\n",
    "print(\"Confusion Matrix:\\n\",classification_report(y_test1,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With CV & with Feature Selection\n",
    "cv_score=cross_val_score(rfc,x1,y1,cv=10)\n",
    "print(\"Accuracy:\",np.mean(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Refinement  of model :  Improving Accuracy of model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1. Removing all Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "df=train.drop(cat_features,axis=1)\n",
    "df.columns\n",
    "#Apply log transformation to deal with skewness of annual income\n",
    "df['annual_inc']=np.log(df['annual_inc'])\n",
    "#train-test split\n",
    "# Split into train and test data\n",
    "x=df.drop('bad_loan',axis=1)\n",
    "y=df.bad_loan\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)\n",
    "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)\n",
    "\n",
    "#Feature Scaling\n",
    "rs=StandardScaler()\n",
    "x_train=rs.fit_transform(x_train)\n",
    "x_test=rs.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without CV \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model=LogisticRegression(solver='liblinear')\n",
    "model.fit(x_train,y_train)\n",
    "y_pred=model.predict(x_test)\n",
    "print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"Classification Report:\\n\",classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=4)\n",
    "pc_x=pca.fit_transform(x)\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(pc_x,y,test_size=0.3,random_state=0)\n",
    "\n",
    "model.fit(x_train,y_train)\n",
    "y_pred=model.predict(x_test)\n",
    "print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"Classification Report:\\n\",classification_report(y_test,y_pred))\n",
    "\n",
    "lr=LogisticRegression()\n",
    "cv_score=cross_val_score(lr,pc_x,y,cv=15)\n",
    "print(np.mean(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Split into train and test data\n",
    "x=train.drop('bad_loan',axis=1)\n",
    "y=train.bad_loan\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)\n",
    "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)\n",
    "\n",
    "#Feature Scaling\n",
    "rs=MinMaxScaler()\n",
    "x_train=rs.fit_transform(x_train)\n",
    "x_test=rs.transform(x_test)\n",
    "\n",
    "#Initialize the Multi Layer Perceptron Classifier\n",
    "#model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)\n",
    "model=MLPClassifier()\n",
    "#Train the model\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "#Predict for the test set\n",
    "y_pred=model.predict(x_test)\n",
    "\n",
    "#Calculate the accuracy of our model\n",
    "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "#Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Apply log transformation to deal with skewness of annual income\n",
    "df1['annual_inc']=np.log(df1['annual_inc'])\n",
    "\n",
    "x=df1.drop('bad_loan',axis=1)\n",
    "y=df1.bad_loan\n",
    "\n",
    "#train-test split\n",
    "# Split into train and test data\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)\n",
    "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)\n",
    "\n",
    "#Feature Scaling\n",
    "rs=StandardScaler()\n",
    "x_train=rs.fit_transform(x_train)\n",
    "x_test=rs.transform(x_test)\n",
    "sm=SMOTE(random_state=12,ratio=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Without CV & without Feature Selection\n",
    "from sklearn.svm import SVC\n",
    "svc=SVC(gamma='auto')\n",
    "svc.fit(x_train,y_train)\n",
    "y_pred=svc.predict(x_test)\n",
    "\n",
    "print(\"Support Vector Machine:\")\n",
    "print (\"SVM:\",accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Without CV & without Feature Selection\n",
    "from sklearn.svm import SVC\n",
    "cv_scores=cross_val_score(svc,x,y,cv=10)\n",
    "print(\"Mean Accuracy:\",np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Without CV & with Feature Selection\n",
    "from sklearn.svm import SVC\n",
    "svc.fit(x_train1,y_train1)\n",
    "y_pred=svc.predict(x_test1)\n",
    "\n",
    "print(\"Support Vector Machine:\")\n",
    "print (\"SVM:\",accuracy_score(y_test1, y_pred))\n",
    "print(confusion_matrix(y_test1,y_pred))\n",
    "print(classification_report(y_test1,y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#With CV & with Feature Selection\n",
    "from sklearn.svm import SVC\n",
    "cv_scores=cross_val_score(svc,x1,y1,cv=10)\n",
    "print(\"Mean Accuracy:\",np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Without CV & without Feature Selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "score=[]\n",
    "for k in range(2,51):\n",
    "    knn=KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train,y_train)\n",
    "    y_pred=knn.predict(x_test)\n",
    "    score.append(accuracy_score(y_pred,y_test))\n",
    "plt.title(\"k vs Accuracy\")\n",
    "plt.plot(range(2,51),score,label='Accuracy')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#with CV & without Feature Selection\n",
    "#Cross Validation\n",
    "model=KNeighborsClassifier(n_neighbors=)\n",
    "cv_score=cross_val_score(model,x,y,cv=10)\n",
    "print(\"Accuracy with Cross Validation:\",np.mean(cv_score))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Without CV & with Feature Selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "score=[]\n",
    "for k in range(2,51):\n",
    "    knn=KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train1,y_train1)\n",
    "    y_pred=knn.predict(x_test1)\n",
    "    score.append(accuracy_score(y_pred,y_test1))\n",
    "plt.title(\"k vs Accuracy\")\n",
    "plt.plot(range(2,51),score,label='Accuracy')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#with CV & with Feature Selection\n",
    "#Cross Validation\n",
    "model=KNeighborsClassifier(n_neighbors=)\n",
    "cv_score=cross_val_score(model,x1,y1,cv=10)\n",
    "print(\"Accuracy with Cross Validation:\",np.mean(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
